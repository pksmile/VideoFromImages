//
//  GenerateVideoAudioFromImages.swift
//  VideoFromImages
//
//  Created by PRAKASH on 6/29/19.
//  Copyright Â© 2019 Prakash. All rights reserved.
//

import UIKit
import AVFoundation

public class GenerateVideoAudioFromImages: NSObject {
    
    
    open class var shared: GenerateVideoAudioFromImages {
        struct Static {
            static var instance = GenerateVideoAudioFromImages()
        }
        
        return Static.instance
    }
    
    
    public enum GenerateVideoAudioType: Int {
        case single, multiple, singleAudioMultipleImage
        
        init() {
            self = .single
        }
    }
    
    
    open var fileName = "movie"
    
    open var videoBackgroundColor: UIColor = UIColor.black
    
    open var scaleWidth: CGFloat?
    
    open var shouldOptimiseImageForVideo: Bool = true
    
    open var maxVideoLengthInSeconds: Double?
    
    open var videoImageWidthForMultipleVideoGeneration = 800
    
    open var videoDurationInSeconds: Double = 0 {
        didSet {
            videoDurationInSeconds = Double(CMTime(seconds: videoDurationInSeconds, preferredTimescale: 1).seconds)
        }
    }
    
    open func generateVideoFromImages(_images: [UIImage], andAudios _audios: [URL], andType _type: GenerateVideoAudioType, _ progress: @escaping ((Progress) -> Void), success: @escaping ((URL) -> Void), failure: @escaping ((Error) -> Void)) {
        let dispatchQueueGenerate = DispatchQueue(label: "generate", qos: .background)
        dispatchQueueGenerate.async { [weak self] in
            GenerateVideoAudioFromImages.shared.initializeValues(withImages: _images, andAudios: _audios, andType: _type)
            
            /// define the input and output size of the video which will be generated by taking the first image's size
            if let firstImage = GenerateVideoAudioFromImages.shared.images.first {
                GenerateVideoAudioFromImages.shared.minSize = firstImage.size
            }
            
            let inputSize = GenerateVideoAudioFromImages.shared.minSize
            let outputSize = GenerateVideoAudioFromImages.shared.minSize
            
            self?.getTempVideoFileUrl { (videoOutputURL) in
                do {
                    /// try to create an asset writer for videos pointing to the video url
                    try GenerateVideoAudioFromImages.shared.videoWriter = AVAssetWriter(outputURL: videoOutputURL, fileType: AVFileType.mp4)
                } catch {
                    GenerateVideoAudioFromImages.shared.videoWriter = nil
                    DispatchQueue.main.async {
                        failure(error)
                    }
                }
                
                /// check if the writer is instantiated successfully
                if let videoWriter = GenerateVideoAudioFromImages.shared.videoWriter {
                    
                    /// create the basic video settings
                    let videoSettings: [String : AnyObject] = [
                        AVVideoCodecKey  : AVVideoCodecH264 as AnyObject,
                        AVVideoWidthKey  : outputSize.width as AnyObject,
                        AVVideoHeightKey : outputSize.height as AnyObject,
                    ]
                    
                    /// create a video writter input
                    let videoWriterInput = AVAssetWriterInput(mediaType: AVMediaType.video, outputSettings: videoSettings)
                    
                    /// create setting for the pixel buffer
                    let sourceBufferAttributes: [String : AnyObject] = [
                        (kCVPixelBufferPixelFormatTypeKey as String): Int(kCVPixelFormatType_32ARGB) as AnyObject,
                        (kCVPixelBufferWidthKey as String): Float(inputSize.width) as AnyObject,
                        (kCVPixelBufferHeightKey as String):  Float(inputSize.height) as AnyObject,
                        (kCVPixelBufferCGImageCompatibilityKey as String): NSNumber(value: true),
                        (kCVPixelBufferCGBitmapContextCompatibilityKey as String): NSNumber(value: true)
                    ]
                    
                    /// create pixel buffer for the input writter and the pixel buffer settings
                    let pixelBufferAdaptor = AVAssetWriterInputPixelBufferAdaptor(assetWriterInput: videoWriterInput, sourcePixelBufferAttributes: sourceBufferAttributes)
                    
                    /// check if an input can be added to the asset
                    assert(videoWriter.canAdd(videoWriterInput))
                    
                    /// add the input writter to the video asset
                    videoWriter.add(videoWriterInput)
                    
                    /// check if a write session can be executed
                    if videoWriter.startWriting() {
                        
                        /// if it is possible set the start time of the session (current at the begining)
                        videoWriter.startSession(atSourceTime: CMTime.zero)
                        
                        /// check that the pixel buffer pool has been created
                        assert(pixelBufferAdaptor.pixelBufferPool != nil)
                        
                        /// create/access separate queue for the generation process
                        let media_queue = DispatchQueue(label: "mediaInputQueue", attributes: [])
                        
                        /// start video generation on a separate queue
                        videoWriterInput.requestMediaDataWhenReady(on: media_queue, using: { () -> Void in
                            
                            /// set up preliminary properties for the image count, frame count and the video elapsed time
                            let numImages = GenerateVideoAudioFromImages.shared.images.count
                            var frameCount = 0
                            var elapsedTime: Double = 0
                            
                            /// calculate the frame duration by dividing the full video duration by the number of images and rounding up the number
                            let frameDuration = CMTime(seconds: ceil(Double(GenerateVideoAudioFromImages.shared.duration / Double(GenerateVideoAudioFromImages.shared.images.count))), preferredTimescale: 1)
                            let currentProgress = Progress(totalUnitCount: Int64(GenerateVideoAudioFromImages.shared.images.count))
                            
                            /// declare a temporary array to hold all as of yet unused images
                            var remainingPhotos = [UIImage](GenerateVideoAudioFromImages.shared.images)
                            
                            var nextStartTimeForFrame: CMTime! = CMTime(seconds: 0, preferredTimescale: 1)
                            var imageForVideo: UIImage!
                            
                            /// if the input writer is ready and we have not yet used all imaged
                            while (videoWriterInput.isReadyForMoreMediaData && frameCount < numImages) {
                                
                                if GenerateVideoAudioFromImages.shared.type == .single {
                                    /// pick the next photo to be loaded
                                    imageForVideo = remainingPhotos.remove(at: 0)
                                    
                                    /// calculate the beggining time of the next frame; if the frame is the first, the start time is 0, if not, the time is the number of the frame multiplied by the frame duration in seconds
                                    nextStartTimeForFrame = frameCount == 0 ? CMTime(seconds: 0, preferredTimescale: 1) : CMTime(seconds: Double(frameCount) * frameDuration.seconds, preferredTimescale: 1)
                                } else {
                                    /// get the right photo from the array
                                    imageForVideo = GenerateVideoAudioFromImages.shared.images[frameCount]
                                    
                                    if GenerateVideoAudioFromImages.shared.type == .multiple {
                                        /// calculate the start of the frame; if the frame is the first, the start time is 0, if not, get the already elapsed time
                                        nextStartTimeForFrame = frameCount == 0 ? CMTime(seconds: 0, preferredTimescale: 1) : CMTime(seconds: Double(elapsedTime), preferredTimescale: 1)
                                        
                                        /// add the max between the audio duration time or a minimum duration to the elapsed time
                                        elapsedTime += GenerateVideoAudioFromImages.shared.audioDurations[frameCount] <= 1 ? GenerateVideoAudioFromImages.shared.minSingleVideoDuration : GenerateVideoAudioFromImages.shared.audioDurations[frameCount]
                                    } else {
                                        nextStartTimeForFrame = frameCount == 0 ? CMTime(seconds: 0, preferredTimescale: 600) : CMTime(seconds: Double(elapsedTime), preferredTimescale: 600)
                                        
                                        let audio_Time = GenerateVideoAudioFromImages.shared.audioDurations[0]
                                        let total_Images = GenerateVideoAudioFromImages.shared.images.count
                                        elapsedTime += audio_Time / Double(total_Images)
                                    }
                                }
                                
                                /// append the image to the pixel buffer at the right start time
                                if !GenerateVideoAudioFromImages.shared.appendPixelBufferForImage(imageForVideo, pixelBufferAdaptor: pixelBufferAdaptor, presentationTime: nextStartTimeForFrame) {
                                    DispatchQueue.main.async {
                                        failure(VideoGeneratorError(error: .kFailedToAppendPixelBufferError))
                                    }
                                }
                                
                                // increise the frame count
                                frameCount += 1
                                
                                currentProgress.completedUnitCount = Int64(frameCount)
                                
                                // after each successful append of an image track the current progress
                                progress(currentProgress)
                            }
                            
                            // after all images are appended the writting shoul be marked as finished
                            videoWriterInput.markAsFinished()
                            
                            if let _maxLength = self?.maxVideoLengthInSeconds {
                                videoWriter.endSession(atSourceTime: CMTime(seconds: _maxLength, preferredTimescale: 1))
                            }
                            
                            // the completion is made with a completion handler which will return the url of the generated video or an error
                            videoWriter.finishWriting { () -> Void in
                                if self?.audioURLs.isEmpty == true {
                                    if let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).first {
                                        let newPath = URL(fileURLWithPath: documentsPath + "\(self?.fileName ?? "").m4v")
                                        self?.deleteFile(pathURL: newPath, completion: {
                                            try FileManager.default.moveItem(at: videoOutputURL, to: newPath)
                                        })
                                        print("finished")
                                        DispatchQueue.main.async {
                                            success(newPath)
                                        }
                                    }
                                } else {
                                    /// if the writing is successfull, go on to merge the video with the audio files
                                    GenerateVideoAudioFromImages.shared.addAudioInVideo(withVideoURL: videoOutputURL, success: { (videoURL) in
                                        print("finished")
                                        success(videoURL)
                                    }, failure: { (error) in
                                        failure(error)
                                    })
                                }
                                
                                GenerateVideoAudioFromImages.shared.videoWriter = nil
                            }
                        })
                    } else {
                        DispatchQueue.main.async {
                            failure(VideoGeneratorError(error: .kFailedToStartAssetWriterError))
                        }
                    }
                } else {
                    DispatchQueue.main.async {
                        failure(VideoGeneratorError(error: .kFailedToStartAssetWriterError))
                    }
                }
            }
        }
        
    }
    open class func mergeVideosInSignleVideo(videoURLs: [URL], andFileName fileName: String, success: @escaping ((URL) -> Void), failure: @escaping ((Error) -> Void)) {
        let acceptableVideoExtensions = ["mov", "mp4", "m4v"]
        let _videoURLs = videoURLs.filter({ !$0.absoluteString.contains(".DS_Store") && acceptableVideoExtensions.contains($0.pathExtension.lowercased()) })
        let _fileName = fileName == "" ? "mergedMovie" : fileName
        
        /// guard against missing URLs
        guard !_videoURLs.isEmpty else {
            DispatchQueue.main.async {
                failure(VideoGeneratorError(error: .kMissingVideoURLs))
            }
            return
        }
        
        var videoAssets: [AVURLAsset] = []
        var completeMoviePath: URL?
        
        for path in _videoURLs {
            if let _url = URL(string: path.absoluteString) {
                videoAssets.append(AVURLAsset(url: _url))
            }
        }
        
        if let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).first {
            /// create a path to the video file
            completeMoviePath = URL(fileURLWithPath: documentsPath).appendingPathComponent("\(_fileName).m4v")
            
            if let completeMoviePath = completeMoviePath {
                if FileManager.default.fileExists(atPath: completeMoviePath.path) {
                    do {
                        /// delete an old duplicate file
                        try FileManager.default.removeItem(at: completeMoviePath)
                    } catch {
                        DispatchQueue.main.async {
                            failure(error)
                        }
                    }
                }
            }
        } else {
            DispatchQueue.main.async {
                failure(VideoGeneratorError(error: .kFailedToFetchDirectory))
            }
        }
        
        let composition = AVMutableComposition()
        
        if let completeMoviePath = completeMoviePath {
            
            /// add audio and video tracks to the composition
            if let videoTrack: AVMutableCompositionTrack = composition.addMutableTrack(withMediaType: AVMediaType.video, preferredTrackID: kCMPersistentTrackID_Invalid), let audioTrack: AVMutableCompositionTrack = composition.addMutableTrack(withMediaType: AVMediaType.audio, preferredTrackID: kCMPersistentTrackID_Invalid) {
                
                var insertTime = CMTime(seconds: 0, preferredTimescale: 1)
                
                /// for each URL add the video and audio tracks and their duration to the composition
                for sourceAsset in videoAssets {
                    do {
                        if let assetVideoTrack = sourceAsset.tracks(withMediaType: .video).first, let assetAudioTrack = sourceAsset.tracks(withMediaType: .audio).first {
                            let frameRange = CMTimeRange(start: CMTime(seconds: 0, preferredTimescale: 1), duration: sourceAsset.duration)
                            try videoTrack.insertTimeRange(frameRange, of: assetVideoTrack, at: insertTime)
                            try audioTrack.insertTimeRange(frameRange, of: assetAudioTrack, at: insertTime)
                            
                            videoTrack.preferredTransform = assetVideoTrack.preferredTransform
                        }
                        
                        insertTime = insertTime + sourceAsset.duration
                    } catch {
                        DispatchQueue.main.async {
                            failure(error)
                        }
                    }
                }
                
                /// try to start an export session and set the path and file type
                if let exportSession = AVAssetExportSession(asset: composition, presetName: AVAssetExportPresetHighestQuality) {
                    exportSession.outputURL = completeMoviePath
                    exportSession.outputFileType = AVFileType.mp4
                    exportSession.shouldOptimizeForNetworkUse = true
                    
                    /// try to export the file and handle the status cases
                    exportSession.exportAsynchronously(completionHandler: {
                        switch exportSession.status {
                        case .failed:
                            if let _error = exportSession.error {
                                DispatchQueue.main.async {
                                    failure(_error)
                                }
                            }
                            
                        case .cancelled:
                            if let _error = exportSession.error {
                                DispatchQueue.main.async {
                                    failure(_error)
                                }
                            }
                            
                        default:
                            print("finished")
                            DispatchQueue.main.async {
                                success(completeMoviePath)
                            }
                        }
                    })
                } else {
                    DispatchQueue.main.async {
                        failure(VideoGeneratorError(error: .kFailedToStartAssetExportSession))
                    }
                }
            }
        }
    }
    
    public override init() {
        super.init()
    }
    
    fileprivate func initializeValues(withImages _images: [UIImage], andAudios _audios: [URL], andType _type: GenerateVideoAudioType) {
        images = []
        audioURLs = []
        audioDurations = []
        duration = 0.0
        var datasImages = [Data?]()
        
        /// guard against missing images or audio
        guard !_images.isEmpty else {
            return
        }
        
        type = _type
        audioURLs = _audios
        
        if self.type == .single {
            if let _image = self.shouldOptimiseImageForVideo ? _images.first?.resizeImageToVideoSize() : _images.first {
                self.images = [UIImage](repeating: _image, count: 2)
            }
        } else {
            
            for _image in _images {
                autoreleasepool {
                    if let imageData = _image.scaleImageToSize(newSize: CGSize(width: videoImageWidthForMultipleVideoGeneration, height: videoImageWidthForMultipleVideoGeneration))?.pngData() {
                        datasImages.append(imageData)
                    }
                }
            }
            
            datasImages.forEach {
                if let imageData = $0, let image = UIImage(data: imageData, scale: UIScreen.main.scale) {
                    self.images.append(image)
                }
            }
            
            datasImages.removeAll()
        }
        
        switch type! {
        case .single, .singleAudioMultipleImage:
            /// guard against multiple audios in single mode
            if _audios.count != 1 {
                if let _audio = _audios.first {
                    audioURLs = [_audio]
                }
            }
        case .multiple:
            /// guard agains more then equal audio and images for multiple
            if _audios.count != _images.count {
                let count = min(_audios.count, _images.count)
                audioURLs = Array(_audios[...(count - 1)])
                
                if !audioURLs.isEmpty {
                    images = Array(_images[...(count - 1)])
                }
            }
        }
        
        var _duration: Double = 0
        
        var audioAssets: [AVURLAsset] = []
        for url in _audios {
            audioAssets.append(AVURLAsset(url: url, options: nil))
        }
        
        /// calculate the full video duration
        for audio in audioAssets {
            if let _maxLength = maxVideoLengthInSeconds {
                _duration += round(Double(CMTimeGetSeconds(audio.duration)))
                
                if _duration < _maxLength {
                    audioDurations.append(round(Double(CMTimeGetSeconds(audio.duration))))
                } else {
                    _duration -= round(Double(CMTimeGetSeconds(audio.duration)))
                    let diff = _maxLength - _duration
                    _duration = _maxLength
                    audioDurations.append(diff)
                }
            } else {
                audioDurations.append(round(Double(CMTimeGetSeconds(audio.duration))))
                _duration += round(Double(CMTimeGetSeconds(audio.duration)))
            }
        }
        
        let minVideoDuration = Double(CMTime(seconds: minSingleVideoDuration, preferredTimescale: 1).seconds)
        duration = max((audioURLs.isEmpty ? videoDurationInSeconds : _duration), minVideoDuration)
        
        if audioURLs.isEmpty {
            audioDurations = [Double](repeating: duration / Double(images.count), count: images.count)
        }
        
        if let _scaleWidth = scaleWidth {
            images = images.compactMap({ $0.scaleImageToSize(newSize: CGSize(width: _scaleWidth, height: _scaleWidth)) })
        }
    }
    fileprivate var images: [UIImage] = []
    
    fileprivate var audioDurations: [Double] = []
    
    fileprivate var audioURLs: [URL] = []
    
    fileprivate var duration: Double! = 1.0
    
    fileprivate var videoWriter: AVAssetWriter?
    
    fileprivate var type: GenerateVideoAudioType?
    
    fileprivate var minSize = CGSize.zero
    
    fileprivate var minSingleVideoDuration: Double = 3.0
    
    fileprivate var reversedVideoURL: URL?
    
    private func addAudioInVideo(withVideoURL videoUrl: URL, success: @escaping ((URL) -> Void), failure: @escaping ((Error) -> Void)) {
        let dispatchQueueMerge = DispatchQueue(label: "merge audio", qos: .background)
        dispatchQueueMerge.async { [weak self] in
            /// create a mutable composition
            let mixComposition = AVMutableComposition()
            
            /// create a video asset from the url and get the video time range
            let videoAsset = AVURLAsset(url: videoUrl, options: nil)
            let videoTimeRange = CMTimeRange(start: CMTime.zero, duration: videoAsset.duration)
            
            /// add a video track to the composition
            let videoComposition = mixComposition.addMutableTrack(withMediaType: AVMediaType.video, preferredTrackID: kCMPersistentTrackID_Invalid)
            
            if let videoTrack = videoAsset.tracks(withMediaType: .video).first {
                do {
                    /// try to insert the video time range into the composition
                    try videoComposition?.insertTimeRange(videoTimeRange, of: videoTrack, at: CMTime.zero)
                } catch {
                    failure(error)
                }
                
                var duration = CMTime(seconds: 0, preferredTimescale: 1)
                
                /// add an audio track to the composition
                let audioCompositon = mixComposition.addMutableTrack(withMediaType: .audio, preferredTrackID: kCMPersistentTrackID_Invalid)
                
                /// for all audio files add the audio track and duration to the existing audio composition
                for (index, audioUrl) in self?.audioURLs.enumerated() ?? [].enumerated() {
                    let audioDuration = CMTime(seconds: self?.audioDurations[index] ?? 0.0, preferredTimescale: 1)
                    
                    let audioAsset = AVURLAsset(url: audioUrl)
                    let audioTimeRange = CMTimeRange(start: CMTime.zero, duration: self?.maxVideoLengthInSeconds != nil ? audioDuration : audioAsset.duration)
                    
                    let shouldAddAudioTrack = self?.maxVideoLengthInSeconds != nil ? audioDuration.seconds > 0 : true
                    
                    if shouldAddAudioTrack {
                        if let audioTrack = audioAsset.tracks(withMediaType: .audio).first {
                            do {
                                try audioCompositon?.insertTimeRange(audioTimeRange, of: audioTrack, at: duration)
                            } catch {
                                failure(error)
                            }
                        }
                    }
                    
                    duration = duration + (self?.maxVideoLengthInSeconds != nil ? audioDuration : audioAsset.duration)
                }
                
                /// check if the documents folder is available
                if let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).first {
                    self?.getTempVideoFileUrl { (_) in }
                    
                    /// create a path to the video file
                    let videoOutputURL = URL(fileURLWithPath: documentsPath).appendingPathComponent("\(self?.fileName ?? "").m4v")
                    self?.deleteFile(pathURL: videoOutputURL) {
                        if let exportSession = AVAssetExportSession(asset: mixComposition, presetName: AVAssetExportPresetHighestQuality) {
                            exportSession.outputURL = videoOutputURL
                            exportSession.outputFileType = AVFileType.mp4
                            exportSession.shouldOptimizeForNetworkUse = true
                            
                            /// try to export the file and handle the status cases
                            exportSession.exportAsynchronously(completionHandler: {
                                if exportSession.status == .failed || exportSession.status == .cancelled {
                                    if let _error = exportSession.error {
                                        DispatchQueue.main.async {
                                            failure(_error)
                                        }
                                    }
                                } else {
                                    DispatchQueue.main.async {
                                        success(videoOutputURL)
                                    }
                                }
                            })
                        } else {
                            DispatchQueue.main.async {
                                failure(VideoGeneratorError(error: .kFailedToStartAssetExportSession))
                            }
                        }
                    }
                } else {
                    DispatchQueue.main.async {
                        failure(VideoGeneratorError(error: .kFailedToFetchDirectory))
                    }
                }
            } else {
                DispatchQueue.main.async {
                    failure(VideoGeneratorError(error: .kFailedToReadVideoTrack))
                }
            }
        }
    }
    private func appendPixelBufferForImage(_ image: UIImage, pixelBufferAdaptor: AVAssetWriterInputPixelBufferAdaptor, presentationTime: CMTime) -> Bool {
        
        /// at the beginning of the append the status is false
        var appendSucceeded = false
        
        /**
         *  The proccess of appending new pixels is put inside a autoreleasepool
         */
        autoreleasepool {
            
            // check posibilitty of creating a pixel buffer pool
            if let pixelBufferPool = pixelBufferAdaptor.pixelBufferPool {
                
                let pixelBufferPointer = UnsafeMutablePointer<CVPixelBuffer?>.allocate(capacity: MemoryLayout<CVPixelBuffer?>.size)
                let status: CVReturn = CVPixelBufferPoolCreatePixelBuffer(
                    kCFAllocatorDefault,
                    pixelBufferPool,
                    pixelBufferPointer
                )
                
                /// check if the memory of the pixel buffer pointer can be accessed and the creation status is 0
                if let pixelBuffer = pixelBufferPointer.pointee, status == 0 {
                    
                    // if the condition is satisfied append the image pixels to the pixel buffer pool
                    fillPixelBufferFromImage(image, pixelBuffer: pixelBuffer)
                    
                    // generate new append status
                    appendSucceeded = pixelBufferAdaptor.append(
                        pixelBuffer,
                        withPresentationTime: presentationTime
                    )
                    
                    /**
                     *  Destroy the pixel buffer contains
                     */
                    pixelBufferPointer.deinitialize(count: 1)
                } else {
                    NSLog("error: Failed to allocate pixel buffer from pool")
                }
                
                /**
                 Destroy the pixel buffer pointer from the memory
                 */
                pixelBufferPointer.deallocate()
            }
        }
        
        return appendSucceeded
    }
    
    private func fillPixelBufferFromImage(_ image: UIImage, pixelBuffer: CVPixelBuffer) {
        // lock the buffer memoty so no one can access it during manipulation
        CVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: CVOptionFlags(0)))
        
        // get the pixel data from the address in the memory
        let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer)
        
        // create a color scheme
        let rgbColorSpace = CGColorSpaceCreateDeviceRGB()
        
        /// set the context size
        let contextSize = image.size
        
        // generate a context where the image will be drawn
        if let context = CGContext(data: pixelData, width: Int(contextSize.width), height: Int(contextSize.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue) {
            
            var imageHeight = image.size.height
            var imageWidth = image.size.width
            
            if Int(imageHeight) > context.height {
                imageHeight = 16 * (CGFloat(context.height) / 16).rounded(.awayFromZero)
            } else if Int(imageWidth) > context.width {
                imageWidth = 16 * (CGFloat(context.width) / 16).rounded(.awayFromZero)
            }
            
            let center = type == .single ? CGPoint.zero : CGPoint(x: (minSize.width - imageWidth) / 2, y: (minSize.height - imageHeight) / 2)
            
            context.clear(CGRect(x: 0.0, y: 0.0, width: imageWidth, height: imageHeight))
            
            // set the context's background color
            context.setFillColor(type == .single ? UIColor.black.cgColor : videoBackgroundColor.cgColor)
            context.fill(CGRect(x: 0.0, y: 0.0, width: CGFloat(context.width), height: CGFloat(context.height)))
            
            context.concatenate(.identity)
            
            // draw the image in the context
            
            if let cgImage = image.cgImage {
                context.draw(cgImage, in: CGRect(x: center.x, y: center.y, width: imageWidth, height: imageHeight))
            }
            
            // unlock the buffer memory
            CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: CVOptionFlags(0)))
        }
    }
    
    private func getTempVideoFileUrl(completion: @escaping (URL) -> ()) {
        DispatchQueue.main.async {
            if let documentsPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).first {
                let testOutputURL = URL(fileURLWithPath: documentsPath).appendingPathComponent("test.m4v")
                do {
                    if FileManager.default.fileExists(atPath: testOutputURL.path) {
                        try FileManager.default.removeItem(at: testOutputURL)
                    }
                } catch {
                    print(error.localizedDescription)
                }
                
                completion(testOutputURL)
            }
        }
    }
    
    
    private func deleteFile(pathURL: URL, completion: @escaping () throws -> ()) {
        DispatchQueue.main.async {
            do {
                if FileManager.default.fileExists(atPath: pathURL.path) {
                    try FileManager.default.removeItem(at: pathURL)
                }
            } catch {
                print(error.localizedDescription)
            }
            
            do {
                try completion()
            } catch {
                print(error.localizedDescription)
            }
        }
    }
}
